#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ÃœrÃ¼n Scraper v3.3 - Database URL Source"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from supabase import create_client
from datetime import datetime
from urllib.parse import urlparse
import hashlib

TEST_LIMIT = 0  # 0 = TÃ¼m Ã¼rÃ¼nleri scrape et

SUPABASE_URL = "https://zmmpuysxnwqngvlafolm.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InptbXB1eXN4bndxbmd2bGFmb2xtIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjkwNjA0MTAsImV4cCI6MjA4NDYzNjQxMH0.4Q7k-cDcaGhOurMlofG8lkd4ApPyYexxkMdXxH-lI0k"

supabase = None

SITE_CONFIGS = {
    'technopluskibris.com': {
        'name': 'TECHNOPLUSKIBRIS',
        'selectors': {
            'title': ['h1.product-name', 'h1.product-title', '.product-detail h1', 'h1', 'title'],
            'price': ['.product-price span', '.product-price', 'span[class*="price"]', 'div[class*="price"]'],
            'currency': 'TL'
        }
    },
    'durmazz.com': {
        'name': 'DURMAZZ',
        'selectors': {
            'title': ['h1[itemprop="name"]', '.product-title h1', 'h1.product-name', 'h1', 'title'],
            'price': ['span[itemprop="price"]', '.oe_currency_value', 'span.oe_price', '.product_price span'],
            'currency': 'USD'
        }
    },
    'irismostore.com': {
        'name': 'IRISMOSTORE',
        'selectors': {
            'title': [
                'h1.productDetail-title',
                'h1.product-name',
                '.product-detail-name',
                'h1[itemprop="name"]',
                'h1',
                '.product-title',
                'title'
            ],
            'price': [
                'span.productDetail-price',
                'div.productDetail-price span',
                '.product-price-value',
                'span.price-value',
                'div.price span',
                'span[class*="price"]',
                '.product-price',
                'h3',
                'span.price',
                'meta[property="product:price:amount"]'
            ],
            'currency': 'USD'  # USD olarak deÄŸiÅŸtirildi
        }
    },
    'sharafstore.com': {
        'name': 'SHARAFSTORE',
        'selectors': {
            'title': ['h1.product-title', 'h1[itemprop="name"]', '.product-name', 'h1', 'title'],
            'price': [
                'span.price',
                '.product-price span',
                'span[class*="price"]',
                'div.price',
                '.price-wrapper span'
            ],
            'currency': 'TL'
        }
    }
}

def generate_sku(url, site_name):
    url_part = url.rstrip('/').split('/')[-1]
    site_prefix = site_name[:3].upper()
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    return f"{site_prefix}-{url_part[:30]}-{url_hash}"

def init_supabase():
    global supabase
    try:
        print("\nğŸ” Supabase baÄŸlantÄ±sÄ± test ediliyor...")
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        result = supabase.table('products').select("count", count='exact').execute()
        count = result.count if hasattr(result, 'count') else 0
        print(f"  âœ… BaÄŸlantÄ± baÅŸarÄ±lÄ±! {count} Ã¼rÃ¼n mevcut")
        return True
    except Exception as e:
        print(f"  âŒ Hata: {e}")
        return False

def get_urls_from_database():
    """productofsitemapcrawl tablosundan URL'leri Ã§eker (processed olmayanlar)"""
    try:
        print("\nğŸ“¥ VeritabanÄ±ndan URL'ler Ã§ekiliyor...")
        
        # Ã–nce processed=false olanlarÄ± kontrol et
        response = supabase.table('productofsitemapcrawl')\
            .select('id, url, anawebsite, processed')\
            .or_('processed.is.null,processed.eq.false')\
            .execute()
        
        # EÄŸer processed kolonu yoksa, tÃ¼m URL'leri Ã§ek
        if not response.data:
            print("  â„¹ï¸ Processed flag yok veya tÃ¼m URL'ler iÅŸlenmiÅŸ, tÃ¼m kayÄ±tlar Ã§ekiliyor...")
            response = supabase.table('productofsitemapcrawl')\
                .select('id, url, anawebsite')\
                .execute()
        
        if response.data:
            processed_count = len([r for r in response.data if r.get('processed') == True])
            unprocessed_count = len(response.data) - processed_count
            
            print(f"  âœ… {len(response.data)} URL bulundu")
            print(f"     â””â”€ Ä°ÅŸlenmemiÅŸ: {unprocessed_count}")
            if processed_count > 0:
                print(f"     â””â”€ Ä°ÅŸlenmiÅŸ: {processed_count} (atlandÄ±)")
            
            return response.data
        else:
            print("  âš ï¸ VeritabanÄ±nda URL bulunamadÄ±")
            return []
    except Exception as e:
        print(f"  âŒ Hata: {e}")
        print(f"     â””â”€ TÃ¼m URL'ler Ã§ekiliyor (fallback)...")
        
        # Hata durumunda tÃ¼m URL'leri Ã§ek
        try:
            response = supabase.table('productofsitemapcrawl')\
                .select('id, url, anawebsite')\
                .execute()
            
            if response.data:
                print(f"  âœ… {len(response.data)} URL bulundu (fallback)")
                return response.data
        except:
            pass
        
        return []

def get_site_config_from_url(url):
    """URL'den site config'ini belirler"""
    domain = urlparse(url).netloc.replace('www.', '')
    
    for config_domain, config in SITE_CONFIGS.items():
        if config_domain in domain:
            return config
    
    # Default config (eÄŸer tanÄ±mlÄ± deÄŸilse)
    return {
        'name': domain.upper().replace('.', ''),
        'selectors': {
            'title': ['h1', '.product-title', 'title'],
            'price': ['.price', 'span.price', '.product-price'],
            'currency': 'TL'
        }
    }

def save_product_to_db(product, site_name):
    try:
        if not supabase:
            return False
        
        sku = generate_sku(product['url'], site_name)
        new_price = product['price']
        
        # Mevcut Ã¼rÃ¼nÃ¼ kontrol et
        existing = supabase.table('products').select('price, previous_price, price_change').eq('sku', sku).execute()
        
        old_price = None
        previous_price = None
        price_change = None
        price_changed_at = None
        is_new_product = False
        
        if existing.data and len(existing.data) > 0:
            # ÃœrÃ¼n VAR - gÃ¼ncelleme yapÄ±lacak
            old_price = existing.data[0].get('price')
            
            if old_price is not None and new_price is not None:
                # Ä°ki fiyat da var, karÅŸÄ±laÅŸtÄ±r
                old_price_float = float(old_price)
                new_price_float = float(new_price)
                
                if old_price_float != new_price_float:
                    # ğŸ¯ FÄ°YAT DEÄÄ°ÅTÄ°!
                    previous_price = old_price
                    price_change = new_price_float - old_price_float
                    price_changed_at = datetime.now().isoformat()
                    
                    change_type = "ğŸ“ˆ ARTTI" if price_change > 0 else "ğŸ“‰ DÃœÅTÃœ"
                    print(f"      ğŸ’° {change_type}: {old_price} â†’ {new_price} ({price_change:+.2f})")
                else:
                    # Fiyat aynÄ± - Ã¶nceki deÄŸerleri koru
                    previous_price = existing.data[0].get('previous_price')
                    price_change = existing.data[0].get('price_change')
                    # price_changed_at gÃ¼ncelleme (Ã¶nceki deÄŸeri koru)
            elif new_price is not None:
                # Ã–nceden fiyat yoktu, ÅŸimdi var
                print(f"      â„¹ï¸ Fiyat eklendi: {new_price}")
            else:
                # Yeni fiyat yok - Ã¶nceki deÄŸerleri koru
                previous_price = existing.data[0].get('previous_price')
                price_change = existing.data[0].get('price_change')
        else:
            # ÃœrÃ¼n YOK - yeni Ã¼rÃ¼n eklenecek
            is_new_product = True
            if new_price is not None:
                print(f"      âœ¨ Yeni Ã¼rÃ¼n: {new_price}")
        
        # Stock status
        stock_status = 'in_stock' if new_price is not None else 'unknown'
        
        # Data hazÄ±rla
        data = {
            'sku': sku,
            'name': product['title'],
            'price': new_price,
            'previous_price': previous_price,
            'price_change': price_change,
            'price_changed_at': price_changed_at,
            'stock_status': stock_status,
            'url': product['url'],
            'product_name': product['title'],
            'product_url': product['url'],
            'stock_data': {
                'site': site_name,
                'currency': product.get('currency'),
                'last_seen_price': new_price,
                'scraped_at': datetime.now().isoformat(),
                'is_new_product': is_new_product,
                'price_history': {
                    'old': str(old_price) if old_price else None,
                    'new': str(new_price) if new_price else None,
                    'change': str(price_change) if price_change else None
                }
            },
            'scraped_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        # VeritabanÄ±na kaydet/gÃ¼ncelle
        result = supabase.table('products').upsert(data, on_conflict='sku').execute()
        
        # Debug: KaydÄ±n baÅŸarÄ±lÄ± olduÄŸunu kontrol et
        if result.data:
            # Sessiz baÅŸarÄ± (sadece fiyat deÄŸiÅŸirse mesaj gÃ¶ster)
            return True
        else:
            print(f"      âš ï¸ Upsert sonucu boÅŸ dÃ¶ndÃ¼ (SKU: {sku})")
            return False
            
    except Exception as e:
        print(f"      âŒ DB HatasÄ±: {str(e)}")
        return False

def mark_url_as_processed(url_id, success=True):
    """URL'yi processed olarak iÅŸaretler"""
    try:
        data = {
            'processed': True,
            'processed_at': datetime.now().isoformat()
        }
        
        # last_scrape_status kolonunu kaldÄ±rdÄ±k (tabloda yok)
        
        supabase.table('productofsitemapcrawl')\
            .update(data)\
            .eq('id', url_id)\
            .execute()
        
        return True
    except Exception as e:
        # Processed kolonu yoksa sessizce devam et
        if 'column' in str(e).lower() and 'processed' in str(e).lower():
            return True  # Kolon yok, sorun deÄŸil
        
        print(f"      âš ï¸ Ä°ÅŸaretleme hatasÄ±: {str(e)[:50]}")
        return False

def extract_price(soup, selectors):
    for selector in selectors:
        try:
            element = soup.select_one(selector)
            if element:
                price_text = None
                
                # 1. Meta tag ise content attribute'undan al
                if element.name == 'meta':
                    price_text = element.get('content', '')
                
                # 2. Data attribute'lerden dene (data-price, data-value vs)
                elif not price_text:
                    for attr in ['data-price', 'data-value', 'data-product-price', 'content']:
                        if element.get(attr):
                            price_text = element.get(attr)
                            break
                
                # 3. Text iÃ§eriÄŸinden al
                if not price_text:
                    price_text = element.get_text(strip=True)
                
                if not price_text:
                    continue
                
                # Fiyat parse
                # VirgÃ¼l ve boÅŸluklarÄ± temizle
                price_text = price_text.replace(',', '').replace(' ', '').replace('\n', '').replace('\t', '')
                
                # Para birimi sembollerini kaldÄ±r (TL, USD, $, â‚¬ vb)
                price_text = price_text.replace('TL', '').replace('USD', '').replace('$', '').replace('â‚¬', '')
                
                # Sadece rakam ve nokta bÄ±rak
                price_text = re.sub(r'[^\d.]', '', price_text)
                
                if price_text:
                    try:
                        price = float(price_text)
                        if price > 0:
                            return price
                    except:
                        continue
        except:
            continue
    return None

def extract_title(soup, selectors):
    for selector in selectors:
        try:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if element.name == 'title':
                    title = re.split(r'\s*[|\-]\s*', title)[0]
                if title and len(title) > 3:
                    return title
        except:
            continue
    return "Bilinmiyor"

def scrape_product(url, config, db_enabled=False):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        title = extract_title(soup, config['selectors']['title'])
        price = extract_price(soup, config['selectors']['price'])
        currency = config['selectors']['currency']
        
        # DEBUG: Fiyat bulunamazsa HTML'den ipucu bul
        if price is None and 'irismostore' in url.lower():
            # Fiyat olabilecek tÃ¼m elementleri bul
            potential_prices = []
            
            # TÃ¼m span, div, meta'larÄ± tara
            for elem in soup.find_all(['span', 'div', 'meta', 'p']):
                text = elem.get_text(strip=True) if elem.name != 'meta' else elem.get('content', '')
                # $ veya rakam iÃ§eren elementleri bul
                if text and ('$' in text or any(char.isdigit() for char in text)):
                    if len(text) < 50:  # Ã‡ok uzun textleri alma
                        potential_prices.append(f"{elem.name}.{elem.get('class', [''])[0] if elem.get('class') else ''}: {text[:30]}")
            
            if potential_prices:
                print(f"\n      ğŸ” Fiyat bulunamadÄ±, potansiyel elementler:")
                for p in potential_prices[:5]:  # Ä°lk 5'ini gÃ¶ster
                    print(f"         {p}")
        
        product_data = {'title': title, 'price': price, 'currency': currency, 'url': url}
        
        if db_enabled:
            db_success = save_product_to_db(product_data, config['name'])
            db_icon = "ğŸ’¾" if db_success else "âš ï¸"
        else:
            db_icon = "ğŸ“"
        
        # Daha detaylÄ± log
        price_str = f"{price} {currency}" if price else "âŒ Fiyat yok"
        title_short = title[:40] + "..." if len(title) > 40 else title
        
        print(f"    {db_icon} {title_short} - {price_str}")
        
        # Fiyat yoksa URL'yi de gÃ¶ster (debug iÃ§in)
        if price is None:
            print(f"       â””â”€ URL: {url[:70]}...")
        
        return product_data
    except requests.exceptions.Timeout:
        print(f"    â±ï¸ Timeout: {url[:50]}...")
        return None
    except requests.exceptions.RequestException as e:
        print(f"    âœ— Network Error: {str(e)[:40]}")
        return None
    except Exception as e:
        print(f"    âœ— Parse Error: {str(e)[:40]}")
        print(f"       â””â”€ URL: {url[:60]}...")
        return None

def scrape_from_database(db_enabled=False):
    """VeritabanÄ±ndan URL'leri Ã§ekip scrape eder"""
    print(f"\n{'='*70}")
    print(f"ğŸ—„ï¸ VERÄ°TABANINDAN SCRAPING")
    print(f"{'='*70}")
    
    # URL'leri veritabanÄ±ndan Ã§ek
    url_records = get_urls_from_database()
    
    if not url_records:
        print("âœ— Ä°ÅŸlenecek URL bulunamadÄ±")
        return []
    
    # Processed olanlarÄ± filtrele
    unprocessed_records = [r for r in url_records if not r.get('processed')]
    
    if not unprocessed_records:
        print("âš ï¸ TÃ¼m URL'ler zaten iÅŸlenmiÅŸ!")
        print("ğŸ’¡ Ä°pucu: Yeniden scrape etmek iÃ§in SQL Ã§alÄ±ÅŸtÄ±rÄ±n:")
        print("   UPDATE productofsitemapcrawl SET processed = false;")
        return []
    
    if TEST_LIMIT > 0:
        print(f"\nâš ï¸ TEST: Ä°lk {TEST_LIMIT} URL")
        unprocessed_records = unprocessed_records[:TEST_LIMIT]
    
    products = []
    site_stats = {}
    
    print(f"\nğŸ“Š {len(unprocessed_records)} URL scrape edilecek")
    print(f"{'â”€'*70}")
    
    for i, record in enumerate(unprocessed_records, 1):
        url_id = record.get('id')
        url = record.get('url')
        ana_website = record.get('anawebsite', '')
        
        if not url:
            continue
        
        # Site config'ini belirle
        config = get_site_config_from_url(url)
        site_name = config['name']
        
        # Site istatistiklerini baÅŸlat
        if site_name not in site_stats:
            site_stats[site_name] = {'total': 0, 'success': 0, 'failed': 0}
        
        print(f"  [{i}/{len(unprocessed_records)}] {site_name}", end=" ")
        
        product = scrape_product(url, config, db_enabled)
        
        if product:
            product['site'] = site_name
            product['anawebsite'] = ana_website
            products.append(product)
            site_stats[site_name]['success'] += 1
            
            # BaÅŸarÄ±lÄ± - processed olarak iÅŸaretle
            if url_id:
                mark_url_as_processed(url_id, success=True)
        else:
            site_stats[site_name]['failed'] += 1
            
            # BaÅŸarÄ±sÄ±z - yine de iÅŸaretle (tekrar denemesin)
            if url_id:
                mark_url_as_processed(url_id, success=False)
        
        site_stats[site_name]['total'] += 1
        
        # Rate limiting
        if i % 10 == 0:
            time.sleep(1)
        else:
            time.sleep(0.3)
    
    print(f"{'â”€'*70}")
    print(f"âœ… {len(products)} Ã¼rÃ¼n tamamlandÄ±")
    
    # Site bazlÄ± Ã¶zet
    print(f"\nğŸ“Š Site BazlÄ± Ã–zet:")
    for site_name, stats in site_stats.items():
        success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
        print(f"  {site_name}: {stats['success']}/{stats['total']} baÅŸarÄ±lÄ± ({success_rate:.1f}%)")
        if stats['failed'] > 0:
            print(f"     â””â”€ BaÅŸarÄ±sÄ±z: {stats['failed']}")
    
    return products

def main():
    print(f"\n{'='*70}")
    print("ğŸš€ SCRAPER v3.3 - DATABASE URL SOURCE")
    print(f"{'='*70}")
    
    db_enabled = init_supabase()
    
    if db_enabled:
        print("ğŸ’¾ JSON + Supabase")
    else:
        print("ğŸ“ Sadece JSON")
    
    # VeritabanÄ±ndan scrape et
    all_products = scrape_from_database(db_enabled)
    
    # JSON'a kaydet
    with open('products.json', 'w', encoding='utf-8') as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)
    
    # Ã–zet istatistikler
    print(f"\n{'='*70}")
    print("ğŸ“Š GENEL Ã–ZET")
    print(f"{'='*70}")
    print(f"Toplam: {len(all_products)}")
    
    total_with_price = len([p for p in all_products if p.get('price') is not None])
    total_without_price = len(all_products) - total_with_price
    
    print(f"FiyatlÄ±: {total_with_price}")
    print(f"FiyatsÄ±z: {total_without_price}")
    
    print(f"\nâœ… products.json kaydedildi")
    if db_enabled:
        print(f"âœ… Supabase gÃ¼ncellendi")
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()
